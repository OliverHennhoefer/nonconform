{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fb27b941602401d91542211134fc71a",
   "metadata": {},
   "source": [
    "# Method Comparison\n",
    "\n",
    "This notebook compares conformal estimation variants on a fixed benchmark dataset.\n",
    "If you are new to this topic, read each section in order and focus on how the decision rule changes while the detector and data stay constant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acae54e37e7d407bbb7b55eff062a284",
   "metadata": {},
   "source": [
    "## Import\n",
    "\n",
    "This section loads all dependencies used throughout the notebook.\n",
    "The probabilistic sections require optional probabilistic extras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a63283cbaf04dbcab1f6479b197f3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import pandas as pd\n",
    "from oddball import Dataset, load\n",
    "from pyod.models.hbos import HBOS\n",
    "from scipy.stats import false_discovery_control\n",
    "\n",
    "from nonconform import ConformalDetector, Empirical, Probabilistic, Split\n",
    "from nonconform.enums import Pruning\n",
    "from nonconform.fdr import weighted_false_discovery_control\n",
    "from nonconform.metrics import false_discovery_rate, statistical_power\n",
    "from nonconform.weighting import (\n",
    "    BootstrapBaggedWeightEstimator,\n",
    "    logistic_weight_estimator,\n",
    ")\n",
    "\n",
    "root_logger = logging.getLogger(\"nonconform\")\n",
    "if not root_logger.handlers:\n",
    "    root_logger.addHandler(logging.NullHandler())\n",
    "root_logger.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd0d8092fe74a7c96281538738b07e2",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "We load the Shuttle benchmark with a deterministic seed for reproducible outputs.\n",
    "Using one fixed dataset helps isolate differences between estimation and selection methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72eea5119410473aa328ad9291626812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train: (22793, 9), x_test: (1000, 9)\n",
      "y_test positives: 100\n",
      "alpha=0.2, calibration size=1000\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_test = load(Dataset.SHUTTLE, setup=True, seed=1)\n",
    "\n",
    "n_calib = 1_000\n",
    "strategy = Split(n_calib=n_calib)\n",
    "alpha = 0.2\n",
    "\n",
    "n_positives = int(y_test.sum())\n",
    "print(f\"x_train: {x_train.shape}, x_test: {x_test.shape}\")\n",
    "print(f\"y_test positives: {n_positives}\")\n",
    "print(f\"alpha={alpha}, calibration size={n_calib}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edb47106e1a46a883d545849b8ab81b",
   "metadata": {},
   "source": [
    "## Method Comparison\n",
    "\n",
    "We evaluate four configurations that combine estimation type and selection rule.\n",
    "Note, that weighted p-values need distinct multiple testing adjustment.\n",
    "\n",
    "1. Standard Empirical + Benjamini-Hochberg Procedure (BH)\n",
    "2. Standard Probabilistic + Benjamini-Hochberg Procedure (BH)\n",
    "3. Weighted Empirical + Weighted Conformal Selection (WCS)\n",
    "4. Weighted Probabilistic + Weighted Conformal Selection (WCS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10185d26023b46108eb7d9f57d49d2b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      method  discoveries    fdr  power\n",
      "     Standard Empirical (BH)          118 0.2034 0.9400\n",
      " Standard Probabilistic (BH)          112 0.1607 0.9400\n",
      "    Weighted Empirical (WCS)          105 0.1048 0.9400\n",
      "Weighted Probabilistic (WCS)          105 0.1048 0.9400\n"
     ]
    }
   ],
   "source": [
    "def summarize_row(name, decisions):\n",
    "    \"\"\"Compute summary metrics for one decision vector.\"\"\"\n",
    "    return {\n",
    "        \"method\": name,\n",
    "        \"discoveries\": int(decisions.sum()),\n",
    "        \"fdr\": float(false_discovery_rate(y=y_test, y_hat=decisions)),\n",
    "        \"power\": float(statistical_power(y=y_test, y_hat=decisions)),\n",
    "    }\n",
    "\n",
    "\n",
    "rows = []\n",
    "\n",
    "# 1) Standard Empirical + BH\n",
    "ce = ConformalDetector(\n",
    "    detector=HBOS(),\n",
    "    strategy=strategy,\n",
    "    estimation=Empirical(),\n",
    "    seed=1,\n",
    ")\n",
    "ce.fit(x_train)\n",
    "p_values = ce.compute_p_values(x_test)\n",
    "decisions = false_discovery_control(p_values, method=\"bh\") <= alpha\n",
    "rows.append(summarize_row(\"Standard Empirical (BH)\", decisions))\n",
    "\n",
    "# 2) Standard Probabilistic + BH\n",
    "pce = ConformalDetector(\n",
    "    detector=HBOS(),\n",
    "    strategy=strategy,\n",
    "    estimation=Probabilistic(n_trials=10),\n",
    "    seed=1,\n",
    ")\n",
    "pce.fit(x_train)\n",
    "p_values = pce.compute_p_values(x_test)\n",
    "decisions = false_discovery_control(p_values, method=\"bh\") <= alpha\n",
    "rows.append(summarize_row(\"Standard Probabilistic (BH)\", decisions))\n",
    "\n",
    "# 3) Weighted Empirical + WCS\n",
    "wce = ConformalDetector(\n",
    "    detector=HBOS(),\n",
    "    strategy=strategy,\n",
    "    weight_estimator=BootstrapBaggedWeightEstimator(\n",
    "        base_estimator=logistic_weight_estimator(),\n",
    "        n_bootstraps=100,\n",
    "    ),\n",
    "    estimation=Empirical(),\n",
    "    seed=1,\n",
    ")\n",
    "wce.fit(x_train)\n",
    "_ = wce.compute_p_values(x_test)\n",
    "decisions = weighted_false_discovery_control(\n",
    "    result=wce.last_result,\n",
    "    alpha=alpha,\n",
    "    pruning=Pruning.DETERMINISTIC,\n",
    "    seed=1,\n",
    ")\n",
    "rows.append(summarize_row(\"Weighted Empirical (WCS)\", decisions))\n",
    "\n",
    "# 4) Weighted Probabilistic + WCS\n",
    "wpce = ConformalDetector(\n",
    "    detector=HBOS(),\n",
    "    strategy=strategy,\n",
    "    weight_estimator=BootstrapBaggedWeightEstimator(\n",
    "        base_estimator=logistic_weight_estimator(),\n",
    "        n_bootstraps=100,\n",
    "    ),\n",
    "    estimation=Probabilistic(n_trials=10),\n",
    "    seed=1,\n",
    ")\n",
    "wpce.fit(x_train)\n",
    "_ = wpce.compute_p_values(x_test)\n",
    "decisions = weighted_false_discovery_control(\n",
    "    result=wpce.last_result,\n",
    "    alpha=alpha,\n",
    "    pruning=Pruning.DETERMINISTIC,\n",
    "    seed=1,\n",
    ")\n",
    "rows.append(summarize_row(\"Weighted Probabilistic (WCS)\", decisions))\n",
    "\n",
    "results = pd.DataFrame(rows)\n",
    "print(results.to_string(index=False, float_format=lambda x: f\"{x:.4f}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8763a12b2bbd4a93a75aff182afb95dc",
   "metadata": {},
   "source": [
    "## Interpretation\n",
    "\n",
    "Use this section to connect each row in the table back to its statistical decision procedure.\n",
    "\n",
    "- Standard rows use Benjamini-Hochberg on p-values.\n",
    "- Weighted rows use Weighted Conformal Selection (WCS) via `weighted_false_discovery_control`.\n",
    "- Probabilistic estimators can be more powerful with smaller calibration sets (<1000),\n",
    "  but they trade the strict finite-sample guarantees of empirical conformal p-values\n",
    "  for asymptotic guarantees.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
